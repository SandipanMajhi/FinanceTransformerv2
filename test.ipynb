{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasetv2 import StockData\n",
    "from BiTransformer import BetaTransformer, FactorTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandipanmajhi/Desktop/Courses/ML_Project/v1 - Copy/vFin/lib64/python3.12/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/sandipanmajhi/Desktop/Courses/ML_Project/v1 - Copy/vFin/lib64/python3.12/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/sandipanmajhi/Desktop/Courses/ML_Project/v1 - Copy/vFin/lib64/python3.12/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "train_dataset = StockData(return_file= \"data/month_ret.pkl\", data_file=\"data/datashare.pkl\", split = \"train\")\n",
    "trainloader = DataLoader(train_dataset, batch_size=256, shuffle= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "beta_model = BetaTransformer(embed_size=128, inner_dim=32, output_dim=5, num_characteristics=94, heads = 8, repeats=5, dropout=0.1)\n",
    "factor_model = FactorTransformer(embed_size=128, inner_dim= 32, output_dim=5, heads=8, num_characteristics=94, repeats=5, dropout=0.1)\n",
    "\n",
    "beta_model.to(device)\n",
    "factor_model.to(device)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "optimizer = torch.optim.Adam([ {\n",
    "    \"params\" : beta_model.parameters(),\n",
    "    \"params\" : factor_model.parameters()\n",
    "}], lr=1e-5)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1140 [00:48<15:17:58, 48.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 51.27640151977539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1140 [01:36<15:13:58, 48.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 35.59489059448242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1140 [02:25<15:20:11, 48.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 34.82820129394531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1140 [03:15<15:30:19, 49.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 26.911029815673828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1140 [04:03<15:22:05, 48.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 45.915565490722656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1140 [04:49<15:06:39, 47.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 39.509220123291016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1140 [05:38<15:06:34, 48.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 62.83960723876953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1140 [06:22<17:11:32, 54.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m b_in \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m att\n\u001b[1;32m     17\u001b[0m betas_out \u001b[38;5;241m=\u001b[39m beta_model(b_in\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32), att\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m---> 18\u001b[0m factors_out \u001b[38;5;241m=\u001b[39m \u001b[43mfactor_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_in\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m return_estimate \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(betas_out, factors_out)\u001b[38;5;241m.\u001b[39msum(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(return_estimate, y\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "File \u001b[0;32m~/Desktop/Courses/ML_Project/v1 - Copy/vFin/lib64/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Courses/ML_Project/v1 - Copy/vFin/lib64/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Courses/ML_Project/v1 - Copy/BiTransformer.py:184\u001b[0m, in \u001b[0;36mFactorTransformer.forward\u001b[0;34m(self, company_characteristics, target_returns, mask)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, company_characteristics : torch\u001b[38;5;241m.\u001b[39mFloatTensor, target_returns : torch\u001b[38;5;241m.\u001b[39mFloatTensor, mask : torch\u001b[38;5;241m.\u001b[39mLongTensor):\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m#### First the encoder interaction ####\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_characteristics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#### Shape : [ N, num_characs, embed_size ]\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     outs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_characteristics):\n",
      "File \u001b[0;32m~/Desktop/Courses/ML_Project/v1 - Copy/vFin/lib64/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Courses/ML_Project/v1 - Copy/vFin/lib64/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Courses/ML_Project/v1 - Copy/BiTransformer.py:138\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, company_characteristics, attention_mask)\u001b[0m\n\u001b[1;32m    135\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(charac_emebeddings)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformer_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTransformer_Blocks:\n\u001b[0;32m--> 138\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded\n",
      "File \u001b[0;32m~/Desktop/Courses/ML_Project/v1 - Copy/vFin/lib64/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Courses/ML_Project/v1 - Copy/vFin/lib64/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Courses/ML_Project/v1 - Copy/BiTransformer.py:94\u001b[0m, in \u001b[0;36mBlockLayer.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, key, value, mask):\n\u001b[0;32m---> 94\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     95\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm1(out1 \u001b[38;5;241m+\u001b[39m query)\n\u001b[1;32m     97\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeedforward(out1))\n",
      "File \u001b[0;32m~/Desktop/Courses/ML_Project/v1 - Copy/vFin/lib64/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Courses/ML_Project/v1 - Copy/vFin/lib64/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Courses/ML_Project/v1 - Copy/BiTransformer.py:59\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# print(f\"Un attention Indices shape = {un_attention_indices}\")\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(un_attention_indices\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m un_attention_indices[index] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m         attention_mask[batch_num][:,index] \u001b[38;5;241m=\u001b[39m k_unattention \n\u001b[1;32m     61\u001b[0m         attention_mask[batch_num][index,:] \u001b[38;5;241m=\u001b[39m q_unattention\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_batch_loss = 0 \n",
    "\n",
    "    for batch in tqdm(trainloader, total=len(trainloader)):\n",
    "        optimizer.zero_grad()\n",
    "        x, att = batch[0]\n",
    "        y = batch[1]\n",
    "\n",
    "        x = x.cuda()\n",
    "        att = att.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        b_in = x + att\n",
    "\n",
    "        betas_out = beta_model(b_in.to(torch.float32), att.to(torch.float32))\n",
    "        factors_out = factor_model(b_in.to(torch.float32), y.to(torch.float32).view(y.shape[0],1), att.to(torch.float32))\n",
    "\n",
    "        return_estimate = torch.mul(betas_out, factors_out).sum(dim = 1)\n",
    "\n",
    "        loss = loss_fn(return_estimate, y.to(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_batch_loss += loss.item()\n",
    "\n",
    "        print(f\"Batch Loss = {loss.item()}\")\n",
    "\n",
    "    print(f\"Epochs {epoch+1}/{epochs} ------------ train loss = {train_batch_loss/len(trainloader)} \")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    b_in = x + att\n",
    "    betas_out = beta_model(b_in.to(torch.float32), att.to(torch.float32))\n",
    "    factors_out = factor_model(b_in.to(torch.float32), y.to(torch.float32).view(y.shape[0],1), att.to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-101.2903,  -83.5545,  -17.8044,  -75.0067,   63.5480,   66.2908,\n",
       "         -33.3369,  -31.6131,  -16.1224,   13.3225, -102.6843,  -48.7030,\n",
       "         -31.0768,  -28.3364,  -50.4019,  -11.7796,  -92.8685,   16.6493,\n",
       "        -109.6773,   26.2689,  -43.2266, -107.6852,  -57.1064,   19.1339,\n",
       "         -86.9978,  -37.3394,   46.9498,   -2.9617,  -48.1317,   78.5096,\n",
       "          26.1454,  -75.6495,  -36.1237, -134.4268,  -57.3510,  -40.1741,\n",
       "         -96.1820,   23.1225,   50.7180,  -27.5669,  -26.1173,  -84.9718,\n",
       "          66.1404,    4.2162,   -9.6041,  -25.3653,   60.8849,  -70.9091,\n",
       "         -89.9695, -236.9128,    2.7734,  -55.5641, -143.4900,  -88.2520,\n",
       "         -15.9793, -113.6538,   41.9622, -171.4710,   62.5981,   -6.6074,\n",
       "         -25.1827, -101.1404,    6.6869,  -46.5135])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(factors_out, betas_out).sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5604,  3.0144, -1.3905,  0.3964, -0.2673], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6039, -0.0195,  0.3782, -0.0153,  0.0196],\n",
       "        [-0.1495, -0.1187,  0.3071,  0.1474,  0.1892],\n",
       "        [-0.1886,  0.0703,  0.3584,  0.0600,  0.2457],\n",
       "        ...,\n",
       "        [-0.6620, -0.2956, -0.1072,  0.3109,  0.1399],\n",
       "        [-0.3750, -0.0899,  0.1466, -0.0107,  0.3157],\n",
       "        [-0.6398, -0.3189,  0.2864, -0.1460,  0.2826]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  4.2783,  -6.0796,  -7.5671,  -0.9493,  -0.6009,   8.0069,  -3.6057,\n",
       "         52.8713,  10.8611,  -0.9257,   4.3477,  -7.5894,  -1.3437,   2.6071,\n",
       "         -0.6516,   0.2848,   1.9722,   8.7976,  -3.3750,  -1.0951,   3.0833,\n",
       "         -5.9760,   1.8406,  -1.2201,  -4.4167,  10.1747,  -3.8214,   2.5086,\n",
       "         25.5232,   2.1310,  13.3093,   1.9972,  -2.7206,  -2.5938,   0.4079,\n",
       "          3.1591,   7.1301,  -4.9559,   2.6740, -10.5948,  -0.0925,  -1.1429,\n",
       "         -2.9951,   8.2406,  10.5608,  -1.8125,   2.1629,  21.1786,  -0.8598,\n",
       "          8.2927,   5.3366,  -4.3224,   3.0287,   4.2767,  -0.0662,   3.8910,\n",
       "         -1.9167,  -0.2500,   5.7159,  -2.1192,   0.7500,   2.4660,   2.1772,\n",
       "          1.5682,   3.9167,  -9.4049,   6.2190,   7.4881,   3.9475,   1.4759,\n",
       "          1.7500,   0.3711,  13.4486,  -3.3750,   0.2130,  12.8887,   7.1859,\n",
       "          2.4674,   9.6879,   1.8777,  14.8443,  -8.8214,  -3.1071,   1.8777,\n",
       "          0.9265,  -0.4722, -14.8703,   2.9758,   3.8618,  -1.0697,  -8.6270,\n",
       "          6.7734,  -0.8524,   6.4885,   9.4355,  -2.9951,  -7.3207,  -4.4022,\n",
       "          0.4963,  -0.2500,  -0.2500,   2.1625,   7.5422,   3.7500,   3.5798,\n",
       "         -4.4873,   4.1618,  -4.4542,  10.2303,   7.0227,   3.6974,  -0.6755,\n",
       "         -3.7519,  -7.9423,  -1.0374,  -1.4884,  -0.2500,  -3.4013,   6.9042,\n",
       "          2.6071, -11.6272,  -0.2500,  -0.2500,   1.2500,  -4.9375,  -1.1429,\n",
       "         -0.2500,  -9.1862,   8.3507,   6.8929,  -3.5519,  -3.7102,  -2.8816,\n",
       "         11.8036,  26.2717,  -0.2500,  -2.5816,   7.3049,   4.0978,   5.5192,\n",
       "         -9.3409,   2.5561,   3.1688,  -1.2256,  11.2004,   0.8082,   1.7302,\n",
       "          8.1416,   0.6632,  -3.0388,   2.6718,  -1.3449,  -0.2500,   1.3893,\n",
       "         15.0726,   1.1075,   1.5815,  -5.6071,  -1.4328,  10.0969,  10.8611,\n",
       "         24.0886,  -0.2500,   4.1748,   3.0040,   0.1453,  -0.2500,  -2.9049,\n",
       "          0.5150,   1.1535,  11.4383,   4.7631,  15.7500,   1.2016,   5.7965,\n",
       "         -1.8424,   2.5735,   7.3130,   1.5970,   0.9846,   1.0344,   5.6759,\n",
       "          9.4274,   0.1737,   4.3275,  -0.9334,  14.4993,   9.1762,  -6.3106,\n",
       "         -2.1731,  12.9575,   6.2248,   3.1768,   3.2174,   6.0000,   8.7952,\n",
       "          1.3436,  17.2353,  -2.0849,   0.7209,  -1.1350,   2.0938,   1.3629,\n",
       "         -4.2104,  -0.2500,   3.1688,   1.6368,   8.4167,   3.8087,  -3.0669,\n",
       "          5.6212,  13.0547,  -0.2500,  -2.1368,  -0.2500,  10.6915, -11.0514,\n",
       "         -0.2500,  10.6989,   0.8253,  -1.9623,   0.6158,   5.3952,   7.0000,\n",
       "          5.6086,   1.3248,   2.6134,   1.2315,   0.9695,  -1.6105,  -0.2500,\n",
       "         -3.0278,  15.5916,  -0.6755,  21.9722,  -0.1007,   6.9356,  -5.3246,\n",
       "          5.2295,   2.2753,   1.0535,  -4.8743,   1.6498,   0.8864,   0.2005,\n",
       "         -5.1044,   2.7070,   5.7845,  -2.1019,  11.0000,   0.9314,   0.1097,\n",
       "          7.5514,  15.3569,  -2.6085,  -0.0638], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.9852e-01,  1.6098e-01,  1.3774e+00,  6.6569e-01, -4.7709e-01,\n",
       "        -1.0528e+00, -5.9948e-01, -4.8443e-01, -1.2229e-01, -1.1031e+00,\n",
       "        -9.2964e-01, -1.2024e+00, -1.3105e+00, -1.7177e-01, -9.5886e-01,\n",
       "         5.3306e-01, -1.1922e+00,  9.6256e-02, -8.1031e-02,  9.2450e-02,\n",
       "         3.4750e-01, -6.2580e-01,  3.8872e-01,  7.3369e-01, -5.5297e-01,\n",
       "        -5.7237e-01, -1.5223e-01, -1.1519e-01,  5.1266e-01,  6.0079e-02,\n",
       "        -1.3270e+00, -1.5696e+00, -1.7195e+00, -1.8946e-01, -5.1987e-01,\n",
       "        -3.6302e-01, -9.5490e-01, -1.7715e+00,  2.1624e-01,  5.4394e-01,\n",
       "        -5.8300e-01, -9.6621e-01, -1.7643e+00, -6.3698e-02, -7.2091e-01,\n",
       "         6.3877e-01, -1.9825e+00, -7.4308e-01,  2.3791e-01,  1.0523e+00,\n",
       "        -1.1094e+00,  1.2581e-01, -9.1685e-01, -1.3387e+00,  2.2675e-02,\n",
       "        -1.4606e-01, -1.0345e+00, -4.4858e-01, -8.8104e-01, -8.9449e-01,\n",
       "        -2.8202e-01, -8.8648e-01, -7.0026e-01, -5.8243e-01, -4.6261e-01,\n",
       "         1.3323e+00, -4.6530e-01, -6.1906e-01, -2.8606e-01, -1.1649e-03,\n",
       "        -1.4933e+00,  5.0567e-01, -2.2273e+00,  4.6009e-01, -1.0336e+00,\n",
       "        -3.3787e-01, -1.0866e+00, -4.7621e-01, -1.5299e-01, -5.8344e-01,\n",
       "         7.2938e-01, -1.1818e+00, -6.7424e-01, -5.8920e-01, -5.1504e-01,\n",
       "        -6.9745e-01, -2.6123e-01, -3.0694e-01,  6.4870e-02, -9.5636e-01,\n",
       "        -2.7367e-01, -8.6735e-01, -6.9497e-01,  8.5761e-01, -3.8408e-01,\n",
       "         4.0255e-01, -1.1529e+00, -5.1647e-01, -9.2219e-01,  1.5411e+00,\n",
       "        -1.1996e+00, -1.7549e+00, -1.0610e+00, -8.6499e-02,  4.2618e-01,\n",
       "         2.2702e-01,  6.4272e-01, -1.5789e+00, -1.2342e+00,  7.2788e-01,\n",
       "        -9.7335e-01, -1.3363e-01,  7.1157e-01, -9.9283e-01, -3.5054e-01,\n",
       "         3.9188e-01, -9.1003e-02, -1.3345e+00,  5.3907e-01, -7.1725e-01,\n",
       "        -3.7396e-01, -9.4272e-01,  7.7707e-01, -2.4838e-01,  1.6016e-01,\n",
       "        -1.1427e+00, -1.5038e+00, -4.0143e-02,  3.4051e-01, -8.2311e-01,\n",
       "        -4.8774e-02,  8.3082e-01, -3.7313e-01, -1.3375e+00, -8.0666e-01,\n",
       "        -1.7954e+00,  2.9928e-01, -8.3537e-01, -8.3114e-01, -2.3906e-01,\n",
       "         4.6761e-01,  9.8449e-02, -4.6789e-01,  1.1425e-01,  7.1365e-01,\n",
       "         1.0355e+00, -4.5404e-01,  5.4754e-02, -6.7386e-01, -1.2138e-01,\n",
       "        -4.1894e-02, -8.2629e-01,  5.5194e-01,  5.2461e-01, -4.2289e-02,\n",
       "        -1.0610e+00,  1.7873e-01, -1.8726e+00,  3.1456e-01, -7.0413e-01,\n",
       "         3.0643e-01, -1.0579e+00,  6.6629e-01,  4.0399e-01, -1.1022e+00,\n",
       "        -8.2942e-01, -6.4488e-01, -5.1276e-01,  6.4494e-02, -1.0706e+00,\n",
       "        -3.5863e-01, -8.1551e-01, -3.2255e-01, -1.0044e-02, -1.7043e-02,\n",
       "        -6.6076e-02, -3.8449e-01,  1.7792e-01,  5.6223e-02, -3.0818e-01,\n",
       "        -1.5220e+00,  2.7377e-01, -9.5627e-01,  3.5037e-01, -5.2767e-01,\n",
       "        -1.3684e+00, -4.6603e-01,  3.0640e-01,  9.2851e-02, -1.0671e-01,\n",
       "         1.4726e-01, -1.0751e+00, -8.3652e-02, -7.5233e-01, -8.9792e-01,\n",
       "         2.8209e-01, -1.1882e+00, -1.9103e+00,  1.0770e-01, -4.9774e-01,\n",
       "        -1.1464e+00, -1.0923e-01, -4.6472e-01,  7.4528e-01, -1.3704e+00,\n",
       "         1.4087e+00, -3.8051e-01,  7.4159e-01, -1.3631e+00, -8.3078e-01,\n",
       "         2.9505e-02,  5.4149e-01, -8.0605e-02, -5.0212e-01, -4.6161e-01,\n",
       "        -9.6248e-01, -9.3135e-01, -7.7963e-02,  5.0402e-01, -7.9203e-01,\n",
       "        -4.7925e-01, -6.9149e-01, -2.0381e-01,  3.3407e-01,  8.4541e-02,\n",
       "        -2.4574e+00, -1.8487e+00, -1.7325e-01, -8.8345e-02, -9.7135e-01,\n",
       "        -3.1569e-01, -7.2202e-01, -8.3618e-01,  6.2005e-01, -5.4364e-01,\n",
       "        -1.4283e+00, -1.6114e+00, -2.4329e-01, -1.7042e+00,  1.5975e-01,\n",
       "         3.4430e-02, -2.2200e+00,  8.4580e-01, -4.4722e-01,  8.2523e-02,\n",
       "        -8.0154e-01, -5.9668e-01,  3.4646e-01,  4.9135e-01, -1.3404e+00,\n",
       "         1.0809e-01,  6.0522e-01, -4.6612e-01, -5.1324e-01, -1.7676e+00,\n",
       "        -2.0139e+00], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.0967e-05)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(betas_out[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "L,V = torch.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5460.0898, 1103.7900, 1069.0416,  989.6089, 1009.4409,  906.0813,\n",
       "         896.9238,  884.9628,  877.9852,  824.7115,  800.4466,  781.9396,\n",
       "         764.2097,  736.5269,  723.3835,  713.1860,  707.3641,  693.6501,\n",
       "         677.9960,  669.5670,  659.3270,  648.9361,  629.7836,  619.5671,\n",
       "         617.3815,  122.4133,  132.0341,  140.2777,  588.2957,  155.5132,\n",
       "         159.6166,  580.1430,  573.9927,  576.4462,  562.1726,  168.6394,\n",
       "         170.0196,  553.8890,  548.8576,  543.8217,  528.7883,  532.6273,\n",
       "         175.7346,  179.9915,  495.3282,  514.6953,  505.4503,  477.2546,\n",
       "         192.5651,  482.3644,  460.6175,  196.0766,  199.6541,  204.5398,\n",
       "         448.8571,  441.8883,  216.1230,  218.0048,  224.7543,  430.9372,\n",
       "         412.5706,  419.3622,  417.6948,  401.9385,  233.5191,  399.0354,\n",
       "         388.3470,  239.8217,  249.6799,  378.3864,  243.3026,  280.9480,\n",
       "         269.9452,  274.2514,  265.8348,  361.5014,  287.8109,  241.9338,\n",
       "         358.5362,  257.7657,  381.8069,  351.0597,  303.0583,  312.2346,\n",
       "         258.2795,  341.6865,  330.2628,  321.0804,  299.3892,  347.4873,\n",
       "         337.1292,  322.0959,  298.9197,  372.5865])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.real(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([94, 94])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4672,  0.6865, -0.5330,  ...,  0.1016, -2.1300,  0.8769],\n",
       "        [ 0.7225, -0.3270, -0.6206,  ...,  2.3793, -0.1092,  0.7760],\n",
       "        [ 1.0716, -1.9624, -0.3132,  ..., -0.7111, -0.9161, -0.6825],\n",
       "        ...,\n",
       "        [ 1.9289, -1.3707, -0.4832,  ...,  1.9094,  1.5193,  0.9237],\n",
       "        [-1.2710,  1.2234,  1.2892,  ..., -0.1634,  1.5330, -0.3174],\n",
       "        [-0.8854, -0.8661, -0.7718,  ..., -1.1532, -1.0024,  0.8124]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vFin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
