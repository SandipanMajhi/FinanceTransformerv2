{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import preprocess as p\n",
    "import pickle as pkl\n",
    "from train import trainer, tester, priorityChecker\n",
    "from modules import FinTransformer\n",
    "from encoder import EncFinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.load(\"trainloader.pt\")\n",
    "testloader = torch.load(\"testloader.pt\")\n",
    "validloader = torch.load(\"validloader.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "inner_dim = 256\n",
    "num_characteristics = 94\n",
    "heads = 8\n",
    "repeats = 4\n",
    "dropout = 0.2\n",
    "batch_size = batch_size\n",
    "num_companies = batch_size\n",
    "\n",
    "model = FinTransformer(embed_size, inner_dim, num_companies, num_characteristics, heads, repeats, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/948 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [02:52<00:00,  5.50it/s]\n",
      "100%|██████████| 1517/1517 [01:07<00:00, 22.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 589.4886899963021 --------- Validation Epoch Loss = 1334.2908217012882\n",
      "Epoch 2/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [02:52<00:00,  5.51it/s]\n",
      "100%|██████████| 1517/1517 [01:07<00:00, 22.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 501.29816084355116 --------- Validation Epoch Loss = 2090.880260258913\n",
      "Epoch 3/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [02:50<00:00,  5.55it/s]\n",
      "100%|██████████| 1517/1517 [01:10<00:00, 21.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 504.2675336971879 --------- Validation Epoch Loss = 1566.616015791893\n",
      "Epoch 4/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [02:49<00:00,  5.60it/s]\n",
      "100%|██████████| 1517/1517 [01:07<00:00, 22.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 506.4651619717479 --------- Validation Epoch Loss = 1904.4837729036808\n",
      "Epoch 5/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [02:48<00:00,  5.64it/s]\n",
      "100%|██████████| 1517/1517 [01:08<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 501.24468340724707 --------- Validation Epoch Loss = 2102.6582977473736\n",
      "Epoch 6/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [02:48<00:00,  5.64it/s]\n",
      "100%|██████████| 1517/1517 [01:07<00:00, 22.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 507.9696453809738 --------- Validation Epoch Loss = 1853.9123616814613\n",
      "Epoch 7/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [02:52<00:00,  5.49it/s]\n",
      " 67%|██████▋   | 1014/1517 [00:47<00:23, 21.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\FinTransformers\\v1\\encoderonlyTest.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/FinTransformers/v1/encoderonlyTest.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer(model \u001b[39m=\u001b[39;49m model, Epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, lr \u001b[39m=\u001b[39;49m \u001b[39m0.01\u001b[39;49m, trainloader \u001b[39m=\u001b[39;49m trainloader, validloader \u001b[39m=\u001b[39;49m validloader, num_companies \u001b[39m=\u001b[39;49m num_companies, num_characteristics \u001b[39m=\u001b[39;49m num_characteristics)\n",
      "File \u001b[1;32md:\\FinTransformers\\v1\\train.py:67\u001b[0m, in \u001b[0;36mtrainer\u001b[1;34m(model, Epochs, lr, trainloader, validloader, checkpoint_path, num_companies, num_characteristics)\u001b[0m\n\u001b[0;32m     63\u001b[0m return_mask \u001b[39m=\u001b[39m data[\u001b[39m5\u001b[39m]\u001b[39m.\u001b[39munsqueeze(dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m(chars\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m num_companies):\n\u001b[1;32m---> 67\u001b[0m     output \u001b[39m=\u001b[39m model(company_characteristics \u001b[39m=\u001b[39;49m chars, return_inputs \u001b[39m=\u001b[39;49m returns, company_mask \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m, return_mask \u001b[39m=\u001b[39;49m return_mask)\n\u001b[0;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     chars_new \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, num_companies \u001b[39m-\u001b[39m chars\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], num_characteristics), device\u001b[39m=\u001b[39m device)\n",
      "File \u001b[1;32mc:\\Users\\sandi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\FinTransformers\\v1\\modules.py:240\u001b[0m, in \u001b[0;36mFinTransformer.forward\u001b[1;34m(self, company_characteristics, return_inputs, company_mask, return_mask)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39mConstraints:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[39m    Mask_token_id = 0 \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39mreturn_inputs : (batch_dates, num_companies, 1)\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39m# self.num_batches = company_ids.shape[0]\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[39m# company_mask = self.create_company_mask(company_mask_ids)\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m# return_mask = self.create_return_mask(return_mask_ids)\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \n\u001b[0;32m    237\u001b[0m \u001b[39m# print(f\"Company mask shape = {company_mask.shape}\")\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m# print(f\"Return Mask shape = {return_mask.shape}\")\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m encoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(company_characteristics, company_mask)\n\u001b[0;32m    241\u001b[0m decoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(encoder_output, return_inputs, return_mask, company_mask)\n\u001b[0;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m decoder_output\n",
      "File \u001b[1;32mc:\\Users\\sandi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\FinTransformers\\v1\\modules.py:133\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, company_characteristics, company_mask)\u001b[0m\n\u001b[0;32m    130\u001b[0m encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(charac_embeds)\n\u001b[0;32m    132\u001b[0m \u001b[39mfor\u001b[39;00m transformer_block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTransformer_Blocks:\n\u001b[1;32m--> 133\u001b[0m     encoded \u001b[39m=\u001b[39m transformer_block(encoded, encoded, encoded, company_mask)\n\u001b[0;32m    134\u001b[0m \u001b[39mreturn\u001b[39;00m encoded\n",
      "File \u001b[1;32mc:\\Users\\sandi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\FinTransformers\\v1\\modules.py:85\u001b[0m, in \u001b[0;36mBlockLayer.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, query, key, value, mask):\n\u001b[1;32m---> 85\u001b[0m     out1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(query, key, value, mask))\n\u001b[0;32m     86\u001b[0m     out1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm1(out1 \u001b[39m+\u001b[39m query)\n\u001b[0;32m     88\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeedforward(out1))\n",
      "File \u001b[1;32mc:\\Users\\sandi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\FinTransformers\\v1\\modules.py:57\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[39m# exp_mask = torch.unsqueeze(mask, dim = 2) \u001b[39;00m\n\u001b[0;32m     56\u001b[0m     qk_product \u001b[39m=\u001b[39m  qk_product\u001b[39m.\u001b[39mmasked_fill(mask \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m), \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-1e28\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m---> 57\u001b[0m qk_probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msoftmax(qk_product, dim \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m)\n\u001b[0;32m     58\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mnqk,nkh->nqh\u001b[39m\u001b[39m\"\u001b[39m,[qk_probs, value_slice])\n\u001b[0;32m     59\u001b[0m \u001b[39m# attention = torch.einsum(\"ndk,nqd->nqk\",[qk_probs, value_slice])\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m# print(attention.shape)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer(model = model, Epochs = 10, lr = 0.0005, trainloader = trainloader, validloader = validloader, num_companies = num_companies, num_characteristics = num_characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = EncFinTransformer(embed_size, inner_dim, num_companies, num_characteristics, heads, repeats, dropout)\n",
    "test_model.load_state_dict(torch.load(\"./checkpoint/best_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4916/4916 [01:53<00:00, 43.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 5600.450335219502\n",
      "R2 Score = -2076091.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tester(model = test_model, testloader = testloader, num_companies = num_companies, num_characteristics = num_characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "915456"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
