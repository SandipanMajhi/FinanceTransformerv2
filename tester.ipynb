{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import preprocess as p\n",
    "import pickle as pkl\n",
    "from train import trainer, tester, priorityChecker\n",
    "from modules import FinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only run when Batch Size changed\n",
    "\n",
    "### Current Notes = train 1985\n",
    "\n",
    "datapath = [\"../x_valid.pkl\", \"../x_train.pkl\", \"../x_test.pkl\"]\n",
    "writepath = [\"validloader.pt\", \"trainloader.pt\", \"testloader.pt\"]\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "for i in range(len(datapath)):\n",
    "    p.transform(datapath=datapath[i], writepath=writepath[i], batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.load(\"trainloader.pt\")\n",
    "testloader = torch.load(\"testloader.pt\")\n",
    "validloader = torch.load(\"validloader.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "inner_dim = 128\n",
    "num_characteristics = 94\n",
    "heads = 8\n",
    "repeats = 3\n",
    "dropout = 0.2\n",
    "batch_size = batch_size\n",
    "num_companies = batch_size\n",
    "\n",
    "model = FinTransformer(embed_size, inner_dim, num_companies, num_characteristics, heads, repeats, dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3224/3224 [07:11<00:00,  7.46it/s]\n",
      "100%|██████████| 629/629 [00:22<00:00, 27.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 376.3765770243481 --------- Validation Epoch Loss = 10.975492258789018\n",
      "Epoch 2/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3224/3224 [07:12<00:00,  7.45it/s]\n",
      "100%|██████████| 629/629 [00:22<00:00, 27.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 205.84326771413907 --------- Validation Epoch Loss = 8.001453612814657\n",
      "Epoch 3/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3224/3224 [07:11<00:00,  7.47it/s]\n",
      "100%|██████████| 629/629 [00:22<00:00, 27.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 155.32071843533777 --------- Validation Epoch Loss = 8.558607965707779\n",
      "Epoch 4/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3224/3224 [07:13<00:00,  7.44it/s]\n",
      "100%|██████████| 629/629 [00:22<00:00, 27.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 117.20002664416097 --------- Validation Epoch Loss = 11.853686579153873\n",
      "Epoch 5/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3224/3224 [07:11<00:00,  7.47it/s]\n",
      "100%|██████████| 629/629 [00:22<00:00, 27.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 103.56075342535041 --------- Validation Epoch Loss = 12.930457103531808\n",
      "Epoch 6/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3224/3224 [07:12<00:00,  7.45it/s]\n",
      "100%|██████████| 629/629 [00:22<00:00, 27.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 95.23625933332369 --------- Validation Epoch Loss = 17.662804819876328\n",
      "Epoch 7/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3224/3224 [07:12<00:00,  7.46it/s]\n",
      "100%|██████████| 629/629 [00:22<00:00, 27.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch Loss = 79.76580753806047 --------- Validation Epoch Loss = 20.884615490213037\n",
      "Epoch 8/10 ---- "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 150/3224 [00:20<06:56,  7.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer(model \u001b[39m=\u001b[39;49m model, Epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, lr \u001b[39m=\u001b[39;49m \u001b[39m0.0002\u001b[39;49m, trainloader \u001b[39m=\u001b[39;49m trainloader, validloader \u001b[39m=\u001b[39;49m validloader, num_companies \u001b[39m=\u001b[39;49m num_companies, num_characteristics \u001b[39m=\u001b[39;49m num_characteristics)\n",
      "File \u001b[1;32md:\\Fin_Transformer\\Modules\\train.py:38\u001b[0m, in \u001b[0;36mtrainer\u001b[1;34m(model, Epochs, lr, trainloader, validloader, checkpoint_path, num_companies, num_characteristics)\u001b[0m\n\u001b[0;32m     35\u001b[0m return_mask \u001b[39m=\u001b[39m data[\u001b[39m5\u001b[39m]\u001b[39m.\u001b[39munsqueeze(dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m(chars\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m num_companies):\n\u001b[1;32m---> 38\u001b[0m     output \u001b[39m=\u001b[39m model(company_characteristics \u001b[39m=\u001b[39;49m chars, return_inputs \u001b[39m=\u001b[39;49m returns, company_mask \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m, return_mask \u001b[39m=\u001b[39;49m return_mask)\n\u001b[0;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     chars_new \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, num_companies \u001b[39m-\u001b[39m chars\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], num_characteristics), device\u001b[39m=\u001b[39m device)\n",
      "File \u001b[1;32mc:\\Users\\sandi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Fin_Transformer\\Modules\\modules.py:240\u001b[0m, in \u001b[0;36mFinTransformer.forward\u001b[1;34m(self, company_characteristics, return_inputs, company_mask, return_mask)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39mConstraints:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[39m    Mask_token_id = 0 \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39mreturn_inputs : (batch_dates, num_companies, 1)\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39m# self.num_batches = company_ids.shape[0]\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[39m# company_mask = self.create_company_mask(company_mask_ids)\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m# return_mask = self.create_return_mask(return_mask_ids)\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \n\u001b[0;32m    237\u001b[0m \u001b[39m# print(f\"Company mask shape = {company_mask.shape}\")\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m# print(f\"Return Mask shape = {return_mask.shape}\")\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m encoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(company_characteristics, company_mask)\n\u001b[0;32m    241\u001b[0m decoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(encoder_output, return_inputs, return_mask, company_mask)\n\u001b[0;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m decoder_output\n",
      "File \u001b[1;32mc:\\Users\\sandi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Fin_Transformer\\Modules\\modules.py:133\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, company_characteristics, company_mask)\u001b[0m\n\u001b[0;32m    130\u001b[0m encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(charac_embeds)\n\u001b[0;32m    132\u001b[0m \u001b[39mfor\u001b[39;00m transformer_block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTransformer_Blocks:\n\u001b[1;32m--> 133\u001b[0m     encoded \u001b[39m=\u001b[39m transformer_block(encoded, encoded, encoded, company_mask)\n\u001b[0;32m    134\u001b[0m \u001b[39mreturn\u001b[39;00m encoded\n",
      "File \u001b[1;32mc:\\Users\\sandi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Fin_Transformer\\Modules\\modules.py:88\u001b[0m, in \u001b[0;36mBlockLayer.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     85\u001b[0m out1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(query, key, value, mask))\n\u001b[0;32m     86\u001b[0m out1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm1(out1 \u001b[39m+\u001b[39m query)\n\u001b[1;32m---> 88\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeedforward(out1))\n\u001b[0;32m     89\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm2(output \u001b[39m+\u001b[39m out1)\n\u001b[0;32m     90\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\sandi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sandi\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\sandi\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer(model = model, Epochs = 10, lr = 0.0002, trainloader = trainloader, validloader = validloader, num_companies = num_companies, num_characteristics = num_characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = FinTransformer(embed_size, inner_dim, num_companies, num_characteristics, heads, repeats, dropout)\n",
    "test_model.load_state_dict(torch.load(\"./checkpoint/best_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1373/1373 [00:51<00:00, 26.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 58.11122443899512\n",
      "R2 Score = 0.9519888162612915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tester(model = test_model, testloader = testloader, num_companies = num_companies, num_characteristics = num_characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1373/1373 [00:49<00:00, 27.92it/s]\n",
      "100%|██████████| 1373/1373 [00:48<00:00, 28.06it/s]\n",
      "100%|██████████| 1373/1373 [00:48<00:00, 28.10it/s]\n",
      "100%|██████████| 1373/1373 [00:48<00:00, 28.12it/s]\n",
      "100%|██████████| 1373/1373 [00:48<00:00, 28.15it/s]\n"
     ]
    }
   ],
   "source": [
    "priorityChecker(model = test_model, testloader = testloader, num_companies = num_companies, num_characteristics = num_characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
